# Practice

```{r setup, include=FALSE}
source("etc/common.R")
```

We have covered a lot in the last few lessons,
so this one presents some practice exercises to ground what we have learned
and introduce a few more commonly-used functions.

## Working with a single tidy table

1. Load the tidyverse collection of libraries and the `here` library for constructing paths:

```{r fake-load-libraries, eval=FALSE}
library(tidyverse)
library(here)
```

2. Use `here::here` to construct a path to a file and `readr::read_csv` to read that file:

```{r read-survey-data}
path = here::here("survey", "person.csv")
person <- readr::read_csv(path)
person
```

*Read `survey/site.csv`.*

3. Count rows and columns using `nrow` and `ncol`:

```{r count-rows-and-columns}
nrow(person)
ncol(person)
```

*How many rows and columns are in the site data?*

4. Format strings using `glue::glue`:

```{r use-glue}
print(glue::glue("person has {nrow(person)} rows and {ncol(person)} columns"))
```

*Print a nicely-formatted summary of the number of rows and columns in the site data.*

5. Use `colnames` to get the names of columns and `paste` to join strings together:

```{r use-colnames-and-paste}
print(glue::glue("person columns are {paste(colnames(person), collapse = ' ')}"))
```

*Print a nicely-formatted summary of the names of the columns in the site data.*

6. Use `dplyr::select` to create a new table with a subset of columns by name:

```{r select-by-name}
dplyr::select(person, family_name, personal_name)
```

*Create a table with just the latitudes and longitudes of sites.*

7. Use `dplyr::filter` to create a new table with a subset of rows by values:

```{r filter-with-one-condition}
dplyr::filter(person, family_name < "M")
```

*Create a table with only sites south of -48 degrees.*

8. Use the pipe operator `%>%` to combine operations:

```{r select-pipe-filter}
person %>%
  dplyr::select(family_name, personal_name) %>%
  dplyr::filter(family_name < "M")
```

*Create a table with only the latitudes and longitudes of sites south of -48 degrees.*

9. Use `dplyr::mutate` to create a new column with calculated values and `stringr::str_length` to calculate string length:

```{r mutate-name-length}
person %>%
  dplyr::mutate(name_length = stringr::str_length(family_name))
```

*Use the built-in function `round` to create a table with latitudes and longitudes rounded to integers.*

10. Use `dplyr::arrange` to order rows and (optionally) `dplyr::desc` to impose descending order:

```{r mutate-and-arrange}
person %>%
  dplyr::mutate(name_length = stringr::str_length(family_name)) %>%
  dplyr::arrange(dplyr::desc(name_length))
```

*Create a table sorted by decreasing longitude (i.e., most negative longitude last).*

## Working with grouped data

1. Read `survey/measurements.csv` and look at the data with `View`:

```{r read-and-view}
measurements <- readr::read_csv(here::here("survey", "measurements.csv"))
View(measurements)
```

2. Find rows where `reading` is not NA and saved as `cleaned`:

```{r remove-reading-na}
cleaned <- measurements %>%
  dplyr::filter(!is.na(reading))
cleaned
```

*Rewrite the filter expression to select rows where the visitor and quantity are not NA either.*

```{r hidden-cleaned, echo=FALSE}
cleaned <- measurements %>%
  dplyr::filter(!is.na(visitor) & !is.na(quantity) & !is.na(reading))
```

3. Group measurements by quantity measured and count the number of each (the column is named `n` automatically):

```{r group-by-quantity}
cleaned %>%
  dplyr::group_by(quantity) %>%
  dplyr::count()
```

*Group by person and quantity measured.*

4. Find the minimum, average, and maximum for each quantity:

```{r min-ave-max}
cleaned %>%
  dplyr::group_by(quantity) %>%
  dplyr::summarize(low = min(reading), mid = mean(reading), high = max(reading))
```

*Look at the range for each combination of person and quantity.*

5. Rescale salinity measurements that are greater than 1:

```{r rescale-salinity}
cleaned <- cleaned %>%
  dplyr::mutate(reading = ifelse(quantity == 'sal' & reading > 1.0, reading/100, reading))
cleaned
```

*Do the same calculation use `case_when`.*

6. Read `visited.csv`, drop the NAs, and join with the cleaned-up table of readings:

```{r join-two-tables}
cleaned <- readr::read_csv(here::here("survey", "visited.csv")) %>%
  dplyr::filter(!is.na(visit_date)) %>%
  dplyr::inner_join(cleaned, by = c("visit_id" = "visit_id"))
cleaned
```

*Join `visited.csv` with `site.csv` to get (date, latitude, longitude) triples for site visits.*

7. Find the dates of the highest radiation reading at each site:

```{r dates-high-rad}
cleaned %>%
  dplyr::filter(quantity == "rad") %>%
  dplyr::group_by(site_id) %>%
  dplyr::mutate(max_rad = max(reading)) %>%
  dplyr::filter(reading == max_rad)
```

Another way to do it:

```{r dates-high-rad-2}
cleaned %>%
  dplyr::filter(quantity == "rad") %>%
  dplyr::group_by(site_id) %>%
  dplyr::top_n(1, reading) %>%
  dplyr::select(site_id, visit_date, reading)
```

*Explain why this __doesn't__ work.*

```{r error-high-rad, error=TRUE}
cleaned %>%
  dplyr::filter(quantity == "rad") %>%
  dplyr::group_by(site_id) %>%
  dplyr::summarize(max_rad = max(reading)) %>%
  dplyr::ungroup() %>%
  dplyr::filter(reading == max_rad)
```

8. Normalize radiation against the highest radiation seen per site:

```{r normalize-rad}
cleaned %>%
  dplyr::filter(quantity == "rad") %>%
  dplyr::group_by(site_id) %>%
  dplyr::mutate(
    max_rad = max(reading),
    frac_rad = reading / max_rad) %>%
  dplyr::select(visit_id, site_id, visit_date, frac_rad)
```

*Normalize salinity against mean salinity by site.*

9. Find stepwise change in radiation per site by date:

```{r rad-change}
cleaned %>%
  dplyr::filter(quantity == "rad") %>%
  dplyr::group_by(site_id) %>%
  dplyr::mutate(delta_rad = reading - dplyr::lag(reading)) %>%
  dplyr::arrange(site_id, visit_date)
```

*Find length of time between visits by site.*

10. Find sites that experience any stepwise increase in radiation between visits:

```{r rad-increases}
cleaned %>%
  dplyr::filter(quantity == "rad") %>%
  dplyr::group_by(site_id) %>%
  dplyr::mutate(delta_rad = reading - dplyr::lag(reading)) %>%
  dplyr::filter(!is.na(delta_rad)) %>%
  dplyr::summarize(any_increase = any(delta_rad > 0)) %>%
  dplyr::filter(any_increase)
```

*Find sites with visits more than one year apart.*

```{r links, child="etc/links.md"}
```
